{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b576d1a5-486a-4b98-a2c6-3f1480aaba3a",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13d3a02d-18a3-42e6-916f-e82f7cde41ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import scipy.sparse\n",
    "import string\n",
    "\n",
    "from ast import literal_eval\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "713065ea-42f1-406e-9ea6-94d498aa2129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CATEGORY = 'Beauty'\n",
    "# CATEGORY = 'DVDs'\n",
    "# CATEGORY = 'Food & Drink'\n",
    "CATEGORY = 'Games'\n",
    "# CATEGORY = 'Internet'\n",
    "\n",
    "signal = 'like'\n",
    "# signal = 'write'\n",
    "# signal = 'both'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f6b7316-6c9b-45d0-ae6a-858879928de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/sise/bshapira-group/lilachzi/models/magnn/data/preprocessed/CiaoGames_like_processed/',\n",
       " '/sise/bshapira-group/lilachzi/models/magnn/data/raw/CiaoGames_like/')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = '/sise/bshapira-group/lilachzi/models/magnn/data'\n",
    "save_prefix = os.path.join(DATA_PATH, f'preprocessed/Ciao{CATEGORY.replace(\" & \", \"_\")}_{signal}_processed/')\n",
    "raw_prefix = os.path.join(DATA_PATH, f'raw/Ciao{CATEGORY.replace(\" & \", \"_\")}_{signal}/')\n",
    "\n",
    "if not os.path.exists(save_prefix):\n",
    "    os.mkdir(save_prefix)\n",
    "if not os.path.exists(raw_prefix):\n",
    "    os.mkdir(raw_prefix)\n",
    "\n",
    "save_prefix, raw_prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea317c64-05b8-405c-84cc-328195b61540",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "410f1286-d871-45a7-926d-b0a5810f0dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSVS_PATH = '/sise/bshapira-group/lilachzi/csvs/'\n",
    "train_set = pd.read_csv(os.path.join(CSVS_PATH, 'train_set.csv'), converters={'y_true': literal_eval})\n",
    "train_set = train_set[train_set['category'] == CATEGORY]\n",
    "train_set.reset_index(inplace=True)\n",
    "\n",
    "val_set = pd.read_csv(os.path.join(CSVS_PATH, 'valid_set.csv'), converters={'y_true': literal_eval})\n",
    "val_set = val_set[val_set['category'] == CATEGORY]\n",
    "val_set.reset_index(inplace=True)\n",
    "\n",
    "test_set = pd.read_csv(os.path.join(CSVS_PATH, 'test_set.csv'), converters={'y_true': literal_eval})\n",
    "test_set = test_set[test_set['category'] == CATEGORY]\n",
    "test_set.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5e2793a-825c-49cf-a1bc-67cc54aa7c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ciao = pd.read_csv(os.path.join(CSVS_PATH, 'ciao_4core.tsv'), sep='\\t')\n",
    "ciao = ciao[ciao['category'] == CATEGORY]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a17c07-4963-44a2-9072-b1cba4790345",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0953ed46-4668-434e-88fe-2c59e2d5fcf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get unique user_ids and product_ids from *all* sets, because some ids exist only in some of the sets\n",
    "users = set(set(train_set['voter_id'].unique()) | set(val_set['voter_id'].unique()) | set(test_set['voter_id'].unique()))\n",
    "products = set(set(train_set['product_id'].unique()) | set(val_set['product_id'].unique()) | set(test_set['product_id'].unique()))\n",
    "\n",
    "reviews = []\n",
    "for row in train_set['y_true']:\n",
    "    reviews.extend(list(row.keys()))\n",
    "for row in val_set['y_true']:\n",
    "    reviews.extend(list(row.keys()))\n",
    "for row in test_set['y_true']:\n",
    "    reviews.extend(list(row.keys()))\n",
    "reviews = set(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13f02124-a07a-4e05-82ea-b4c353616d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users: 4269\n",
      "Reviews: 8919\n",
      "Products: 909\n"
     ]
    }
   ],
   "source": [
    "num_users = len(users)\n",
    "num_reviews = len(reviews)\n",
    "num_products = len(products)\n",
    "\n",
    "print(f'Users: {num_users}')\n",
    "print(f'Reviews: {num_reviews}')\n",
    "print(f'Products: {num_products}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ef25a4e-af4d-4ca4-96d7-1e627005c87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_mapping(nodes):\n",
    "    # id2idx = mapping betweem the node original id and its sorted index\n",
    "    # idx2id = mapping between the sorted index and the corresponsing original id\n",
    "    id2idx, idx2id = {}, {}\n",
    "\n",
    "    for idx, node in enumerate(nodes):\n",
    "        id2idx[node] = idx\n",
    "        idx2id[idx] = node\n",
    "\n",
    "    return id2idx, idx2id\n",
    "\n",
    "users_id2idx, users_idx2id = node_mapping(users)\n",
    "reviews_id2idx, reviews_idx2id = node_mapping(reviews)\n",
    "products_id2idx, products_idx2id = node_mapping(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8704d4a4-268c-413a-8217-bd919a50388b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = {\n",
    "    'umap': users_idx2id,\n",
    "    'rmap': reviews_idx2id,\n",
    "    'pmap': products_idx2id\n",
    "}\n",
    "\n",
    "with open(os.path.join(save_prefix, 'mappings.pickle'), 'wb') as f:\n",
    "    pickle.dump(mappings, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd09bd6-347f-4332-b326-16973efcf346",
   "metadata": {},
   "source": [
    "## Build user-review interactions and negative samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101bf081-cc41-406f-ae1e-32cef34213a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### User-review interaction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679d2fb3-737b-44c3-8095-629cfa3f5f4c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Liked interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a9df9d6a-8284-4e2a-a98e-959e1e649917",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "like train interactions: 100%|██████████| 242500/242500 [00:12<00:00, 19694.98it/s]\n",
      "valid interactions: 100%|██████████| 81173/81173 [00:03<00:00, 23758.28it/s]\n",
      "test interactions: 100%|██████████| 81402/81402 [00:03<00:00, 23839.27it/s]\n"
     ]
    }
   ],
   "source": [
    "user_review, review_product, user_product = [], set(), set()\n",
    "idx_dict = {'train': [], 'valid': [], 'test': []}\n",
    "positive_vote = [3, 4, 5]\n",
    "\n",
    "train_set['voter_idx'] = train_set['voter_id'].map(users_id2idx)\n",
    "train_set['product_idx'] = train_set['product_id'].map(products_id2idx)\n",
    "\n",
    "count = 0\n",
    "for _, row in tqdm(train_set.iterrows(), total=len(train_set), desc='like train interactions'):\n",
    "    user_idx = row['voter_idx']\n",
    "    product_idx = row['product_idx']\n",
    "    \n",
    "    # if the user liked one of the reviews in the product, add user-product interaction\n",
    "    if any([r in row['y_true'].values() for r in positive_vote]):\n",
    "        user_product.add((user_idx, product_idx))\n",
    "        \n",
    "    for review_id, vote in row['y_true'].items():\n",
    "        review_idx = reviews_id2idx[review_id]\n",
    "        \n",
    "        review_product.add((review_idx, product_idx))\n",
    "        \n",
    "        if vote in positive_vote:    \n",
    "            user_review.append((user_idx, review_idx))\n",
    "            idx_dict['train'].append(count)\n",
    "            count += 1\n",
    "            \n",
    "val_set['voter_idx'] = val_set['voter_id'].map(users_id2idx)\n",
    "val_set['product_idx'] = val_set['product_id'].map(products_id2idx)\n",
    "\n",
    "for _, row in tqdm(val_set.iterrows(), total=len(val_set), desc='valid interactions'):\n",
    "    user_idx = row['voter_idx']\n",
    "    product_idx = row['product_idx']\n",
    "        \n",
    "    for review_id, vote in row['y_true'].items():\n",
    "        review_idx = reviews_id2idx[review_id]\n",
    "        \n",
    "        if vote in positive_vote:    \n",
    "            user_review.append((user_idx, review_idx))\n",
    "            idx_dict['valid'].append(count)\n",
    "            count += 1\n",
    "            \n",
    "test_set['voter_idx'] = test_set['voter_id'].map(users_id2idx)\n",
    "test_set['product_idx'] = test_set['product_id'].map(products_id2idx)\n",
    "\n",
    "for _, row in tqdm(test_set.iterrows(), total=len(test_set), desc='test interactions'):\n",
    "    user_idx = row['voter_idx']\n",
    "    product_idx = row['product_idx']\n",
    "    \n",
    "    for review_id, vote in row['y_true'].items():\n",
    "        review_idx = reviews_id2idx[review_id]\n",
    "        \n",
    "        if vote in positive_vote:    \n",
    "            user_review.append((user_idx, review_idx))\n",
    "            idx_dict['test'].append(count)\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffc9d73-cc0d-4dd2-afb7-acba81373f45",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Write interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7429c3a8-de67-4a15-9b0d-c29f9343bdf7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "write train interactions: 100%|██████████| 27964/27964 [00:06<00:00, 4404.06it/s]\n",
      "valid interactions: 100%|██████████| 81173/81173 [00:03<00:00, 23384.65it/s]\n",
      "test interactions: 100%|██████████| 81402/81402 [00:03<00:00, 23296.65it/s]\n"
     ]
    }
   ],
   "source": [
    "user_review, review_product, user_product = [], set(), set()\n",
    "idx_dict = {'train': [], 'valid': [], 'test': []}\n",
    "positive_vote = [3, 4, 5]\n",
    "\n",
    "ciao['user_idx'] = ciao['user_id'].map(users_id2idx)\n",
    "ciao['review_idx'] = ciao['review_id'].map(reviews_id2idx)\n",
    "ciao['product_idx'] = ciao['product_id'].map(products_id2idx)\n",
    "\n",
    "count = 0\n",
    "for _, row in tqdm(ciao.iterrows(), total=len(ciao), desc='write train interactions'):\n",
    "    if row['user_id'] not in list(users):\n",
    "        continue\n",
    "        \n",
    "    user_idx = row['user_idx']\n",
    "    review_idx = row['review_idx']\n",
    "    product_idx = row['product_idx']\n",
    "    \n",
    "    user_review.append((user_idx, review_idx))\n",
    "    review_product.add((review_idx, product_idx))\n",
    "    user_product.add((user_idx, product_idx))\n",
    "    idx_dict['train'].append(count)\n",
    "    count += 1\n",
    "    \n",
    "val_set['voter_idx'] = val_set['voter_id'].map(users_id2idx)\n",
    "val_set['product_idx'] = val_set['product_id'].map(products_id2idx)\n",
    "\n",
    "for _, row in tqdm(val_set.iterrows(), total=len(val_set), desc='valid interactions'):\n",
    "    user_idx = row['voter_idx']\n",
    "    product_idx = row['product_idx']\n",
    "    \n",
    "    for review_id, vote in row['y_true'].items():\n",
    "        review_idx = reviews_id2idx[review_id]\n",
    "        \n",
    "        if vote in positive_vote:    \n",
    "            user_review.append((user_idx, review_idx))\n",
    "            idx_dict['valid'].append(count)\n",
    "            count += 1\n",
    "            \n",
    "test_set['voter_idx'] = test_set['voter_id'].map(users_id2idx)\n",
    "test_set['product_idx'] = test_set['product_id'].map(products_id2idx)\n",
    "\n",
    "for _, row in tqdm(test_set.iterrows(), total=len(test_set), desc='test interactions'):\n",
    "    user_idx = row['voter_idx']\n",
    "    product_idx = row['product_idx']\n",
    "        \n",
    "    for review_id, vote in row['y_true'].items():\n",
    "        review_idx = reviews_id2idx[review_id]\n",
    "        \n",
    "        if vote in positive_vote:    \n",
    "            user_review.append((user_idx, review_idx))\n",
    "            idx_dict['test'].append(count)\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e028402-8b0c-4212-8829-c57b9dc0317d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Both interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a8ec6f85-5eb6-417b-974a-6ca0f5a73b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "like train interactions: 100%|██████████| 145090/145090 [00:07<00:00, 19170.95it/s]\n",
      "write train interactions: 100%|██████████| 12084/12084 [00:02<00:00, 5960.60it/s]\n",
      "valid interactions: 100%|██████████| 48240/48240 [00:02<00:00, 23348.60it/s]\n",
      "test interactions: 100%|██████████| 48588/48588 [00:02<00:00, 23269.41it/s]\n"
     ]
    }
   ],
   "source": [
    "user_review, author_review, review_product, user_product = [], [], set(), set()\n",
    "idx_dict = {'train': [], 'valid': [], 'test': []}\n",
    "positive_vote = [3, 4, 5]\n",
    "\n",
    "count = 0\n",
    "# Like interactions\n",
    "train_set['voter_idx'] = train_set['voter_id'].map(users_id2idx)\n",
    "train_set['product_idx'] = train_set['product_id'].map(products_id2idx)\n",
    "for _, row in tqdm(train_set.iterrows(), total=len(train_set), desc='like train interactions'):\n",
    "    user_idx = row['voter_idx']\n",
    "    product_idx = row['product_idx']\n",
    "    \n",
    "    # if the user liked one of the reviews in the product, add user-product interaction\n",
    "    if any([r in row['y_true'].values() for r in positive_vote]):\n",
    "        user_product.add((user_idx, product_idx))\n",
    "        \n",
    "    for review_id, vote in row['y_true'].items():\n",
    "        review_idx = reviews_id2idx[review_id]\n",
    "        \n",
    "        review_product.add((review_idx, product_idx))\n",
    "        \n",
    "        if vote in positive_vote:    \n",
    "            user_review.append((user_idx, review_idx))\n",
    "            idx_dict['train'].append(count)\n",
    "            count += 1\n",
    "# Write interactions\n",
    "ciao['user_idx'] = ciao['user_id'].map(users_id2idx)\n",
    "ciao['review_idx'] = ciao['review_id'].map(reviews_id2idx)\n",
    "ciao['product_idx'] = ciao['product_id'].map(products_id2idx)\n",
    "for _, row in tqdm(ciao.iterrows(), total=len(ciao), desc='write train interactions'):\n",
    "    if row['user_id'] not in list(users):\n",
    "        continue\n",
    "        \n",
    "    user_idx = row['user_idx']\n",
    "    review_idx = row['review_idx']\n",
    "    product_idx = row['product_idx']\n",
    "    \n",
    "    author_review.append((user_idx, review_idx))\n",
    "    review_product.add((review_idx, product_idx))\n",
    "    user_product.add((user_idx, product_idx))\n",
    "    \n",
    "val_set['voter_idx'] = val_set['voter_id'].map(users_id2idx)\n",
    "val_set['product_idx'] = val_set['product_id'].map(products_id2idx)\n",
    "    \n",
    "for _, row in tqdm(val_set.iterrows(), total=len(val_set), desc='valid interactions'):\n",
    "    user_idx = row['voter_idx']\n",
    "    product_idx = row['product_idx']\n",
    "        \n",
    "    for review_id, vote in row['y_true'].items():\n",
    "        review_idx = reviews_id2idx[review_id]\n",
    "        \n",
    "        if vote in positive_vote:    \n",
    "            user_review.append((user_idx, review_idx))\n",
    "            idx_dict['valid'].append(count)\n",
    "            count += 1\n",
    "            \n",
    "test_set['voter_idx'] = test_set['voter_id'].map(users_id2idx)\n",
    "test_set['product_idx'] = test_set['product_id'].map(products_id2idx)\n",
    "\n",
    "for _, row in tqdm(test_set.iterrows(), total=len(test_set), desc='test interactions'):\n",
    "    user_idx = row['voter_idx']\n",
    "    product_idx = row['product_idx']\n",
    "        \n",
    "    for review_id, vote in row['y_true'].items():\n",
    "        review_idx = reviews_id2idx[review_id]\n",
    "        \n",
    "        if vote in positive_vote:    \n",
    "            user_review.append((user_idx, review_idx))\n",
    "            idx_dict['test'].append(count)\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a675f2-823b-4d66-a959-5526c653c7c4",
   "metadata": {},
   "source": [
    "### Negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "07de8282-fda6-4705-bdc2-131ba4533029",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_review = np.array(user_review).astype(int)\n",
    "review_product = list(review_product)\n",
    "user_product = list(user_product)\n",
    "\n",
    "train_idx = idx_dict['train']\n",
    "val_idx = idx_dict['valid']\n",
    "test_idx = idx_dict['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c3dc40ba-5d47-4450-b221-ddc070cba25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_product_map = ciao[['review_id', 'product_id']].set_index('review_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3799e86f-2c06-4174-ba66-0fadd9b357d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 145090/145090 [00:01<00:00, 88613.39it/s] \n",
      "100%|██████████| 48240/48240 [00:00<00:00, 86159.01it/s]\n",
      "100%|██████████| 48588/48588 [00:00<00:00, 85739.53it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_negative_reviews(user_idx, y_true, pos_candidates):\n",
    "    y_true = {reviews_id2idx[r_id]: rate for r_id, rate in y_true.items()}\n",
    "    neg_y_true = {r_idx: rate for r_idx, rate in y_true.items() if (user_idx, r_idx) not in pos_candidates}\n",
    "    \n",
    "    return neg_y_true\n",
    "\n",
    "if signal == 'like':\n",
    "    train_pos_candidates = set(map(tuple, user_review[train_idx]))\n",
    "    val_pos_candidates = set(map(tuple, user_review[val_idx]))\n",
    "    test_pos_candidates = set(map(tuple, user_review[test_idx]))\n",
    "elif signal == 'write':\n",
    "    train_set['voter_idx'] = train_set['voter_id'].map(users_id2idx)\n",
    "    \n",
    "    train_pos_candidates = set(map(tuple, user_review[train_idx]))\n",
    "    val_pos_candidates = set(map(tuple, user_review[val_idx]))\n",
    "    test_pos_candidates = set(map(tuple, user_review[test_idx]))\n",
    "elif signal == 'both':\n",
    "    train_pos_candidates = np.concatenate([user_review[train_idx], np.array(author_review).astype(int)])\n",
    "    train_pos_candidates = set(map(tuple, train_pos_candidates))\n",
    "    val_pos_candidates = set(map(tuple, user_review[val_idx]))\n",
    "    test_pos_candidates = set(map(tuple, user_review[test_idx]))\n",
    "    \n",
    "neg_train_set = train_set.copy()\n",
    "neg_train_set['y_true'] = neg_train_set.progress_apply(\n",
    "    lambda r: get_negative_reviews(r['voter_idx'], r['y_true'], train_pos_candidates), axis=1\n",
    ")\n",
    "\n",
    "neg_val_set = val_set.copy()\n",
    "neg_val_set['y_true'] = neg_val_set.progress_apply(\n",
    "    lambda r: get_negative_reviews(r['voter_idx'], r['y_true'], val_pos_candidates), axis=1\n",
    ")\n",
    "\n",
    "neg_test_set = test_set.copy()\n",
    "neg_test_set['y_true'] = neg_test_set.progress_apply(\n",
    "    lambda r: get_negative_reviews(r['voter_idx'], r['y_true'], test_pos_candidates), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60160845-1ef2-4241-99e1-717a1e3fbbdd",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Product review set negative sampling\n",
    "\n",
    "For each user-review positive interaction, we will take random negative interaction from the same product set review, related to relevant review_id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "073c1738-81de-4774-a303-244caa580e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 219026/219026 [01:49<00:00, 2008.10it/s]\n",
      "100%|██████████| 48240/48240 [00:01<00:00, 26147.65it/s]\n",
      "100%|██████████| 48588/48588 [00:01<00:00, 26187.39it/s]\n"
     ]
    }
   ],
   "source": [
    "train_neg_candidates = []\n",
    "new_train_idx = []\n",
    "for idx, (user_idx, review_idx) in tqdm(enumerate(train_pos_candidates), total=len(train_pos_candidates)):\n",
    "    review_id = reviews_idx2id[review_idx]\n",
    "    product_id = review_product_map.loc[review_id]['product_id']\n",
    "    \n",
    "    try:\n",
    "        neg_reviews = neg_train_set[\n",
    "            (neg_train_set['voter_idx'] == user_idx) & \n",
    "            (neg_train_set['product_id'] == product_id)\n",
    "        ]['y_true'].values[0]\n",
    "        \n",
    "        r_idx = random.choice(list(neg_reviews.keys()))\n",
    "        train_neg_candidates.append((user_idx, r_idx))\n",
    "        new_train_idx.append(idx)\n",
    "    except:\n",
    "        if signal == 'write':\n",
    "            neg_reviews = neg_train_set[\n",
    "                (neg_train_set['voter_idx'] == user_idx) & \n",
    "                (neg_train_set['product_id'] == product_id)\n",
    "            ]\n",
    "            if neg_reviews.empty:\n",
    "                neg_reviews = neg_train_set[(neg_train_set['product_id'] == product_id)]['y_true'].values[0]\n",
    "                r_idx = random.choice(list(neg_reviews.keys()))\n",
    "                train_neg_candidates.append((user_idx, r_idx))\n",
    "                new_train_idx.append(idx)\n",
    "        \n",
    "        continue\n",
    "        \n",
    "val_neg_candidates = []\n",
    "for idx, row in tqdm(neg_val_set.iterrows(), total=len(neg_val_set)):\n",
    "    user_idx = row['voter_idx']\n",
    "    neg_reviews = row['y_true']\n",
    "    \n",
    "    for r_idx in neg_reviews.keys():\n",
    "        val_neg_candidates.append((user_idx, r_idx))\n",
    "        \n",
    "test_neg_candidates = []\n",
    "for idx, row in tqdm(neg_test_set.iterrows(), total=len(neg_test_set)):\n",
    "    user_idx = row['voter_idx']\n",
    "    neg_reviews = row['y_true']\n",
    "    \n",
    "    for r_idx in neg_reviews.keys():\n",
    "        test_neg_candidates.append((user_idx, r_idx))\n",
    "        \n",
    "np.savez(save_prefix + 'train_val_test_neg_user_review.npz',\n",
    "         train_neg_user_review=train_neg_candidates,\n",
    "         val_neg_user_review=val_neg_candidates,\n",
    "         test_neg_user_review=test_neg_candidates)\n",
    "np.savez(save_prefix + 'train_val_test_pos_user_review.npz',\n",
    "         train_pos_user_review=user_review[new_train_idx],\n",
    "         val_pos_user_review=user_review[val_idx],\n",
    "         test_pos_user_review=user_review[test_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec4fa79-e410-41f5-b179-5cbe828ff2d1",
   "metadata": {},
   "source": [
    "## Build adjacency matrix and metapaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "28478783-e12b-4d6e-9bbb-2a86042ed28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_review = pd.DataFrame(user_review, columns=['user_idx', 'review_idx'])\n",
    "user_review = user_review.loc[new_train_idx].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "if signal == 'both':\n",
    "    author_review = pd.DataFrame(np.array(list(author_review)).astype(int), columns=['author_idx', 'review_idx'])\n",
    "review_product = pd.DataFrame(np.array(review_product).astype(int), columns=['review_idx', 'product_idx'])\n",
    "user_product = pd.DataFrame(np.array(user_product).astype(int), columns=['user_idx', 'product_idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "37ad15ee-4776-4f5e-964b-c8d3545474be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 208850/208850 [00:06<00:00, 33648.18it/s]\n",
      "100%|██████████| 11620/11620 [00:00<00:00, 35405.72it/s]\n",
      "100%|██████████| 12084/12084 [00:00<00:00, 32901.35it/s]\n",
      "100%|██████████| 152874/152874 [00:04<00:00, 35329.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# build the adjacency matrix\n",
    "# 0 for user, 1 for review, 2 for product\n",
    "dim = num_users + num_reviews + num_products\n",
    "\n",
    "type_mask = np.zeros((dim), dtype=int)\n",
    "type_mask[num_users:num_users+num_reviews] = 1\n",
    "type_mask[num_users+num_reviews:] = 2\n",
    "\n",
    "adj_mat = np.zeros((dim, dim), dtype=int)\n",
    "for _, row in tqdm(user_review.iterrows(), total=len(user_review)):\n",
    "    uid = row['user_idx']\n",
    "    rid = num_users + row['review_idx']\n",
    "    adj_mat[uid, rid] = 1\n",
    "    adj_mat[rid, uid] = 1\n",
    "if signal == 'both':\n",
    "    for _, row in tqdm(author_review.iterrows(), total=len(author_review)):\n",
    "        aid = row['author_idx']\n",
    "        rid = num_users + row['review_idx']\n",
    "        adj_mat[aid, rid] = 2\n",
    "        adj_mat[rid, aid] = 2\n",
    "for _, row in tqdm(review_product.iterrows(), total=len(review_product)):\n",
    "    rid = num_users + row['review_idx']\n",
    "    pid = num_users + num_reviews + row['product_idx']\n",
    "    adj_mat[rid, pid] = 1\n",
    "    adj_mat[pid, rid] = 1\n",
    "for _, row in tqdm(user_product.iterrows(), total=len(user_product)):\n",
    "    uid = row['user_idx']\n",
    "    pid = num_users + num_reviews + row['product_idx']\n",
    "    adj_mat[uid, pid] = 1\n",
    "    adj_mat[pid, uid] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b4464731-e1a8-42dc-b912-04b02fd589f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 208850/208850 [00:00<00:00, 424315.57it/s]\n",
      "100%|██████████| 11620/11620 [00:00<00:00, 168474.48it/s]\n",
      "100%|██████████| 12084/12084 [00:00<00:00, 177994.00it/s]\n",
      "100%|██████████| 152874/152874 [00:00<00:00, 249152.34it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(raw_prefix, 'user_review.dat'), 'w') as f:\n",
    "    for line in tqdm(user_review.to_numpy().astype(str)):\n",
    "        line = '\\t'.join(line)\n",
    "        f.write(f\"{line}\\n\")\n",
    "if signal == 'both':\n",
    "    with open(os.path.join(raw_prefix, 'author_review.dat'), 'w+') as f:\n",
    "        for line in tqdm(author_review.to_numpy().astype(str)):\n",
    "            line = '\\t'.join(line)\n",
    "            f.write(f\"{line}\\n\")\n",
    "with open(os.path.join(raw_prefix, 'review_product.dat'), 'w') as f:\n",
    "    for line in tqdm(review_product.to_numpy().astype(str)):\n",
    "        line = '\\t'.join(line)\n",
    "        f.write(f\"{line}\\n\")\n",
    "with open(os.path.join(raw_prefix, 'user_product.dat'), 'w') as f:\n",
    "    for line in tqdm(user_product.to_numpy().astype(str)):\n",
    "        line = '\\t'.join(line)\n",
    "        f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fc7f7223-4dec-46e4-8772-383b54f49640",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_review_list = {i: adj_mat[i, num_users:num_users+num_reviews].nonzero()[0] for i in range(num_users)}\n",
    "review_user_list = {i: adj_mat[num_users + i, :num_users].nonzero()[0] for i in range(num_reviews)}\n",
    "review_product_list = {i: adj_mat[num_users + i, num_users+num_reviews:].nonzero()[0] for i in range(num_reviews)}\n",
    "product_review_list = {i: adj_mat[num_users + num_reviews + i, num_users:num_users+num_reviews].nonzero()[0] for i in range(num_products)}\n",
    "user_product_list = {i: adj_mat[i, num_users+num_reviews:].nonzero()[0] for i in range(num_users)}\n",
    "product_user_list = {i: adj_mat[num_users+num_reviews+i, :num_users].nonzero()[0] for i in range(num_products)}\n",
    "\n",
    "if signal == 'both':\n",
    "    author_review_list = {i: np.where(adj_mat[i, num_users:num_users+num_reviews] == 2)[0] for i in range(num_users)}\n",
    "    review_author_list = {i: np.where(adj_mat[num_users + i, :num_users] == 2)[0] for i in range(num_reviews)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "28332bbe-e2dd-4eb8-a674-0f2d420b3842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0-1-0\n",
    "def create_u_r_u():\n",
    "    print('Create u-r-u list')\n",
    "    u_r_u = []\n",
    "    for r, u_list in tqdm(review_user_list.items()):\n",
    "        u_r_u.extend([(u1, r, u2) for u1 in u_list for u2 in u_list])\n",
    "    u_r_u = np.array(u_r_u)\n",
    "    u_r_u[:, 1] += num_users\n",
    "    sorted_index = sorted(list(range(len(u_r_u))), key=lambda i : u_r_u[i, [0, 2, 1]].tolist())\n",
    "    u_r_u = u_r_u[sorted_index]\n",
    "    return u_r_u\n",
    "\n",
    "# 1-2-1\n",
    "def create_r_p_r():\n",
    "    print('Create r-p-r list')\n",
    "    r_p_r = []\n",
    "    for p, r_list in tqdm(product_review_list.items()):\n",
    "        r_p_r.extend([(r1, p, r2) for r1 in r_list for r2 in r_list])\n",
    "    r_p_r = np.array(r_p_r)\n",
    "    r_p_r += num_users\n",
    "    r_p_r[:, 1] += num_reviews\n",
    "    sorted_index = sorted(list(range(len(r_p_r))), key=lambda i : r_p_r[i, [0, 2, 1]].tolist())\n",
    "    r_p_r = r_p_r[sorted_index]\n",
    "    return r_p_r\n",
    "\n",
    "#0-1-2-1-0\n",
    "def create_u_r_p_r_u():\n",
    "    print('Create u-r-p-r-u list')\n",
    "    u_r_p_r_u = []\n",
    "    r_p_r = create_r_p_r()\n",
    "    for r1, p, r2 in tqdm(r_p_r):\n",
    "        if len(review_user_list[r1 - num_users]) == 0 or len(review_user_list[r2 - num_users]) == 0:\n",
    "            continue\n",
    "        candidate_u1_list = np.random.choice(len(review_user_list[r1 - num_users]), int(0.2 * len(review_user_list[r1 - num_users])), replace=False)\n",
    "        candidate_u1_list = review_user_list[r1 - num_users][candidate_u1_list]\n",
    "        candidate_u2_list = np.random.choice(len(review_user_list[r2 - num_users]), int(0.2 * len(review_user_list[r2 - num_users])), replace=False)\n",
    "        candidate_u2_list = review_user_list[r2 - num_users][candidate_u2_list]\n",
    "        u_r_p_r_u.extend([(u1, r1, p, r2, u2) for u1 in candidate_u1_list for u2 in candidate_u2_list])\n",
    "    u_r_p_r_u = np.array(u_r_p_r_u)\n",
    "    sorted_index = sorted(list(range(len(u_r_p_r_u))), key=lambda i : u_r_p_r_u[i, [0, 4, 1, 2, 3]].tolist())\n",
    "    u_r_p_r_u = u_r_p_r_u[sorted_index]\n",
    "    return u_r_p_r_u\n",
    "\n",
    "# For writers version, where for each user/review there is only one matching review/user\n",
    "def create_a_r_p_r_a():\n",
    "    print('Create u-r-p-r-u list')\n",
    "    u_r_p_r_u = []\n",
    "    r_p_r = create_r_p_r()\n",
    "    for r1, p, r2 in r_p_r:\n",
    "        if len(review_user_list[r1 - num_users]) == 0 or len(review_user_list[r2 - num_users]) == 0:\n",
    "            continue\n",
    "        candidate_u1_list = review_user_list[r1 - num_users]\n",
    "        candidate_u2_list = review_user_list[r2 - num_users]\n",
    "        u_r_p_r_u.extend([(u1, r1, p, r2, u2) for u1 in candidate_u1_list for u2 in candidate_u2_list])\n",
    "    u_r_p_r_u = np.array(u_r_p_r_u)\n",
    "    sorted_index = sorted(list(range(len(u_r_p_r_u))), key=lambda i : u_r_p_r_u[i, [0, 4, 1, 2, 3]].tolist())\n",
    "    u_r_p_r_u = u_r_p_r_u[sorted_index]\n",
    "    return u_r_p_r_u\n",
    "\n",
    "# 1-0-1\n",
    "def create_r_u_r():\n",
    "    print('Create r-u-r list')\n",
    "    r_u_r = []\n",
    "    for u, r_list in tqdm(user_review_list.items()):\n",
    "        r_u_r.extend([(r1, u, r2) for r1 in r_list for r2 in r_list])\n",
    "    r_u_r = np.array(r_u_r)\n",
    "    r_u_r[:, [0, 2]] += num_users\n",
    "    sorted_index = sorted(list(range(len(r_u_r))), key=lambda i : r_u_r[i, [0, 2, 1]].tolist())\n",
    "    r_u_r = r_u_r[sorted_index]\n",
    "    return r_u_r\n",
    "\n",
    "def create_r_a_r():\n",
    "    print('Create r-u-r list')\n",
    "    r_u_r = []\n",
    "    for u, r_list in user_review_list.items():\n",
    "        r_u_r.extend([(r1, u, r2) for r1 in r_list for r2 in r_list])\n",
    "    r_u_r = np.array(r_u_r)\n",
    "    r_u_r[:, [0, 2]] += num_users\n",
    "    # sorted_index = sorted(list(range(len(r_u_r))), key=lambda i : r_u_r[i, [0, 2, 1]].tolist())\n",
    "    # r_u_r = r_u_r[sorted_index]\n",
    "    return r_u_r\n",
    "\n",
    "# 0-2-0\n",
    "def create_u_p_u():\n",
    "    print('Create u-p-u list')\n",
    "    u_p_u = []\n",
    "    for p, u_list in tqdm(product_user_list.items()):\n",
    "        u_p_u.extend([(u1, p, u2) for u1 in u_list for u2 in u_list])\n",
    "    u_p_u = np.array(u_p_u)\n",
    "    u_p_u[:, [1]] += num_users + num_reviews\n",
    "    sorted_index = sorted(list(range(len(u_p_u))), key=lambda i : u_p_u[i, [0, 2, 1]].tolist())\n",
    "    u_p_u = u_p_u[sorted_index]\n",
    "    return u_p_u\n",
    "\n",
    "# 2-0-2\n",
    "def create_p_u_p():\n",
    "    print('Create p-u-p list')\n",
    "    p_u_p = []\n",
    "    for u, p_list in tqdm(user_product_list.items()):\n",
    "        p_u_p.extend([(p1, u, p2) for p1 in p_list for p2 in p_list])\n",
    "    p_u_p = np.array(p_u_p)\n",
    "    p_u_p[:, [0, 2]] += num_users + num_reviews\n",
    "    sorted_index = sorted(list(range(len(p_u_p))), key=lambda i : p_u_p[i, [0, 2, 1]].tolist())\n",
    "    p_u_p = p_u_p[sorted_index]\n",
    "    return p_u_p\n",
    "\n",
    "# # 1-2-0-2-1\n",
    "def create_r_p_u_p_r():\n",
    "    print('Create r-p-u-p-r list')\n",
    "    offset = num_users+num_reviews\n",
    "    r_p_u_p_r = []\n",
    "    p_u_p = create_p_u_p()\n",
    "    for p1, u, p2 in tqdm(p_u_p):\n",
    "        if len(product_user_list[p1 - offset]) == 0 or len(product_user_list[p2 - offset]) == 0:\n",
    "            continue\n",
    "        candidate_r1_list = np.random.choice(len(product_review_list[p1 - offset]), int(0.2 * len(product_review_list[p1 - offset])), replace=False)\n",
    "        candidate_r1_list = product_review_list[p1 - offset][candidate_r1_list]\n",
    "        candidate_r2_list = np.random.choice(len(product_review_list[p2 - offset]), int(0.2 * len(product_review_list[p2 - offset])), replace=False)\n",
    "        candidate_r2_list = product_review_list[p2 - offset][candidate_r2_list]\n",
    "        r_p_u_p_r.extend([(r1, p1, u, p2, r2) for r1 in candidate_r1_list for r2 in candidate_r2_list])\n",
    "    r_p_u_p_r = np.array(r_p_u_p_r)\n",
    "    sorted_index = sorted(list(range(len(r_p_u_p_r))), key=lambda i : r_p_u_p_r[i, [0, 4, 1, 2, 3]].tolist())\n",
    "    r_p_u_p_r = r_p_u_p_r[sorted_index]\n",
    "    return r_p_u_p_r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778454bc-e2d9-44df-8958-01e6486d1bb5",
   "metadata": {},
   "source": [
    "Metapaths:\n",
    "* (0, 1, 0) = u_r_u: two users who interacted (liked) same review\n",
    "* (0, 1, 2, 1, 0) = u_r_p_r_u: two users who interacted with two reviews under same product\n",
    "* (0, 2, 0) = u_p_u: two users who interacted with reviews under same product\n",
    "* (1, 0, 1) = r_u_r: two reviews who got interactions from same user \n",
    "* (1, 2, 1) = r_p_r: two reviews under same product\n",
    "* (1, 2, 0, 2, 1) = r_p_u_p_r: two reviews under 2 products who got interactions from same user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b5002cd5-2ad0-4b57-8dee-ddf18dc13a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "if signal == 'like':\n",
    "    expected_metapaths = [\n",
    "        [(0, 1, 0), (0, 1, 2, 1, 0), (0, 2, 0)],\n",
    "        [(1, 0, 1), (1, 2, 1), (1, 2, 0, 2, 1)]\n",
    "    ]\n",
    "    # create the directories if they do not exist\n",
    "    for i in range(len(expected_metapaths)):\n",
    "        pathlib.Path(save_prefix + '{}'.format(i)).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    metapath_indices_mapping = {(0, 1, 0): create_u_r_u,\n",
    "                                (0, 1, 2, 1, 0): create_u_r_p_r_u,\n",
    "                                (0, 2, 0): create_u_p_u, \n",
    "                                (1, 0, 1): create_r_u_r,\n",
    "                                (1, 2, 1): create_r_p_r,\n",
    "                                (1, 2, 0, 2, 1): create_r_p_u_p_r}\n",
    "elif signal == 'write':\n",
    "    expected_metapaths = [\n",
    "        [(0, 1, 2, 1, 0), (0, 2, 0)],\n",
    "        [(1, 0, 1), (1, 2, 1), (1, 2, 0, 2, 1)]\n",
    "    ]\n",
    "    # create the directories if they do not exist\n",
    "    for i in range(len(expected_metapaths)):\n",
    "        pathlib.Path(save_prefix + '{}'.format(i)).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    metapath_indices_mapping = {(0, 1, 2, 1, 0): create_a_r_p_r_a,\n",
    "                                (0, 2, 0): create_u_p_u, \n",
    "                                (1, 2, 1): create_r_p_r,\n",
    "                                (1, 0, 1): create_r_a_r,\n",
    "                                (1, 2, 0, 2, 1): create_r_p_u_p_r}\n",
    "    \n",
    "elif signal == 'both':\n",
    "    expected_metapaths = [\n",
    "    [(0, 1, 0), (0, 1, 2, 1, 0), (3, 1, 2, 1, 3), (0, 2, 0)],\n",
    "        [(1, 0, 1), (3, 0, 3), (1, 2, 1), (1, 2, 0, 2, 1)]\n",
    "    ]\n",
    "    # create the directories if they do not exist\n",
    "    for i in range(len(expected_metapaths)):\n",
    "        pathlib.Path(save_prefix + '{}'.format(i)).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    metapath_indices_mapping = {(0, 1, 0): create_u_r_u,\n",
    "                                (0, 1, 2, 1, 0): create_u_r_p_r_u,\n",
    "                                (3, 1, 2, 1, 3): create_a_r_p_r_a,\n",
    "                                (0, 2, 0): create_u_p_u, \n",
    "                                (1, 0, 1): create_r_u_r,\n",
    "                                (3, 0, 3): create_r_a_r,\n",
    "                                (1, 2, 1): create_r_p_r,\n",
    "                                (1, 2, 0, 2, 1): create_r_p_u_p_r}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "246af297-bb88-413d-a5e2-e9a81dc3a5a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create u-r-u list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14091/14091 [00:01<00:00, 13188.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create u-r-p-r-u list\n",
      "Create r-p-r list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1813/1813 [00:00<00:00, 24286.89it/s]\n",
      "100%|██████████| 175081/175081 [00:04<00:00, 35272.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create u-r-p-r-u list\n",
      "Create r-p-r list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1813/1813 [00:00<00:00, 23953.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create u-p-u list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1813/1813 [00:03<00:00, 553.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create r-u-r list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4417/4417 [00:14<00:00, 305.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create r-u-r list\n",
      "Create r-p-r list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1813/1813 [00:00<00:00, 29275.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create r-p-u-p-r list\n",
      "Create p-u-p list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4417/4417 [00:04<00:00, 974.53it/s] \n",
      "100%|██████████| 31703371/31703371 [14:15<00:00, 37059.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# write all things\n",
    "target_idx_lists = [np.arange(num_users), np.arange(num_reviews), np.arange(num_products)]\n",
    "offset_list = [0, num_users, num_products]\n",
    "for i, metapaths in enumerate(expected_metapaths):\n",
    "    for metapath in metapaths:\n",
    "        edge_metapath_idx_array = metapath_indices_mapping[metapath]()\n",
    "        \n",
    "        with open(save_prefix + '{}/'.format(i) + '-'.join(map(str, metapath)) + '_idx.pickle', 'wb') as out_file:\n",
    "            target_metapaths_mapping = {}\n",
    "            left = 0\n",
    "            right = 0\n",
    "            for target_idx in target_idx_lists[i]:\n",
    "                while right < len(edge_metapath_idx_array) and edge_metapath_idx_array[right, 0] == target_idx + offset_list[i]:\n",
    "                    right += 1\n",
    "                target_metapaths_mapping[target_idx] = edge_metapath_idx_array[left:right, ::-1]\n",
    "                left = right\n",
    "            pickle.dump(target_metapaths_mapping, out_file)\n",
    "        \n",
    "        with open(save_prefix + '{}/'.format(i) + '-'.join(map(str, metapath)) + '.adjlist', 'w') as out_file:\n",
    "            left = 0\n",
    "            right = 0\n",
    "            for target_idx in target_idx_lists[i]:\n",
    "                while right < len(edge_metapath_idx_array) and edge_metapath_idx_array[right, 0] == target_idx + offset_list[i]:\n",
    "                    right += 1\n",
    "                neighbors = edge_metapath_idx_array[left:right, -1] - offset_list[i]\n",
    "                neighbors = list(map(str, neighbors))\n",
    "                if len(neighbors) > 0:\n",
    "                    out_file.write('{} '.format(target_idx) + ' '.join(neighbors) + '\\n')\n",
    "                else:\n",
    "                    out_file.write('{}\\n'.format(target_idx))\n",
    "                left = right\n",
    "\n",
    "scipy.sparse.save_npz(save_prefix + 'adj_mat.npz', scipy.sparse.csr_matrix(adj_mat))\n",
    "np.save(save_prefix + 'node_types.npy', type_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96baa6af-09e0-4b7d-8fdc-26ae97196f23",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cf1925c-df33-4357-80f1-141e9ee7fb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_ciao = pd.read_csv('/sise/bshapira-group/lilachzi/csvs/ciao_4core.tsv', sep='\\t')\n",
    "# clean_ciao = pd.read_csv('/sise/bshapira-group/lilachzi/csvs/ciao_4core_summary.tsv', sep='\\t')\n",
    "clean_ciao = clean_ciao[clean_ciao['category'] == CATEGORY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd253bbe-dbe2-4c14-ab00-a2166654010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tokens_with_punctuation(tokens):\n",
    "    punctuation_pattern = re.compile(r'[^\\w\\s]|\\s{2,}')\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        sub_tokens = punctuation_pattern.split(token)\n",
    "        for sub_token in sub_tokens:\n",
    "            new_tokens.append(sub_token)\n",
    "    return new_tokens\n",
    "\n",
    "def filter_tokens(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuation = set(string.punctuation) | {'*', '&'}\n",
    "    alphabetic_pattern = re.compile(r'[a-zA-Z]+')\n",
    "    tokens = [token.lower() for token in tokens if token not in punctuation and alphabetic_pattern.fullmatch(token)]\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def tokenize_text(text, tokenizer='word'):\n",
    "    \"\"\"\n",
    "    Preprocess and tokenize the text. \n",
    "\n",
    "    The preprocessing includes removal of punctuaion marks, numbers and stop words. \n",
    "    The return value if list of filtered tokens or sentences. \n",
    "    \"\"\"\n",
    "    # Remove words with repeating characters\n",
    "    repeat_pattern = re.compile(r'([^\\W\\d_])\\1{2,}')\n",
    "    text = re.sub(repeat_pattern, lambda x: x.group()[0] * 2, text)\n",
    "    \n",
    "    # Tokenize and split tokens with punctuation\n",
    "    sentences = sent_tokenize(text)\n",
    "    sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "    sentences = [split_tokens_with_punctuation(tokens) for tokens in sentences]\n",
    "\n",
    "    # Remove stop words, punctuation and non-alphabetic characters\n",
    "    sentences = [filter_tokens(tokens) for tokens in sentences]\n",
    "    \n",
    "    # Return wanted structure based on tokenizer parameter\n",
    "    if tokenizer == 'word':\n",
    "        return list(itertools.chain(*sentences))\n",
    "    else:\n",
    "        return [' '.join(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9f7cf3-8dba-4702-b2e5-6605080cf8ea",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "838f1b89-7219-43cb-8718-fe9ccf79df1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = KeyedVectors.load('../../PersonzalizedReviews/w2v_ciao_amazon/w2v_ciao_amazon.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0d43e737-674f-492a-b39b-d8fd35e86c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12084/12084 [03:51<00:00, 52.26it/s]\n"
     ]
    }
   ],
   "source": [
    "clean_ciao['tokens'] = clean_ciao['clean_review'].progress_apply(tokenize_text)\n",
    "# clean_ciao['tokens'] = clean_ciao['summary'].progress_apply(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "16c1b98a-12db-4f7d-855b-23ec9c63625e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12084/12084 [00:04<00:00, 2690.53it/s]\n"
     ]
    }
   ],
   "source": [
    "def w2v_vectorize(words):\n",
    "    vectors = []\n",
    "    \n",
    "    for word in words:\n",
    "        if len(word) == 1:\n",
    "            continue\n",
    "        try:    \n",
    "            vectors.append(w2v_model[word])\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return np.average(vectors, axis=0)\n",
    "\n",
    "clean_ciao['embedding'] = clean_ciao['tokens'].progress_apply(w2v_vectorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bf778724-3a40-49a7-834c-2f55aca5ad6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4410/4410 [00:01<00:00, 2414.40it/s]\n"
     ]
    }
   ],
   "source": [
    "user_features = []\n",
    "for user_idx, user_id in tqdm(users_idx2id.items()):\n",
    "    embeddings = np.array(clean_ciao[clean_ciao['user_id'] == user_id]['embedding'].tolist())\n",
    "    \n",
    "    if len(embeddings) == 0:\n",
    "        embeddings = np.zeros((300, ))\n",
    "        user_features.append(embeddings)\n",
    "        continue\n",
    "        \n",
    "    user_features.append(np.mean(embeddings, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "32094299-f147-4f7e-96e2-5a93a5bfdc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12084/12084 [00:04<00:00, 2526.45it/s]\n"
     ]
    }
   ],
   "source": [
    "review_features = []\n",
    "for review_idx, review_id in tqdm(reviews_idx2id.items()):\n",
    "    embedding = clean_ciao[clean_ciao['review_id'] == review_id]['embedding'].values[0]\n",
    "    review_features.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "893a4628-c8fd-4922-a098-57e3711dbc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1318/1318 [00:00<00:00, 2229.05it/s]\n"
     ]
    }
   ],
   "source": [
    "product_features = []\n",
    "for product_idx, product_id in tqdm(products_idx2id.items()):\n",
    "    embeddings = np.array(clean_ciao[clean_ciao['product_id'] == product_id]['embedding'].tolist())\n",
    "    product_features.append(np.mean(embeddings, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0892d795-e08c-4ee3-8ba8-2f8acf0a30ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(save_prefix + 'w2v_implicit_features.npz',\n",
    "         user_features=user_features,\n",
    "         review_features=review_features,\n",
    "         product_features=product_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9c47259-4729-41f2-a0fe-c6b4fb5cfe20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/sise/bshapira-group/lilachzi/models/magnn/data/preprocessed/CiaoBeauty_like_processed/'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3359146c-8706-47e1-96a7-48e5affcbf0e",
   "metadata": {},
   "source": [
    "## User Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cf39f69-d9da-46cf-8433-c58b624134f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = KeyedVectors.load('../../PersonzalizedReviews/w2v_ciao_amazon/w2v_ciao_amazon.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd848c52-1c0d-415e-8551-a232e8348c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8919/8919 [01:43<00:00, 86.37it/s] \n",
      "100%|██████████| 3805/3805 [00:16<00:00, 226.99it/s]\n"
     ]
    }
   ],
   "source": [
    "clean_ciao['tokens'] = clean_ciao['clean_review'].progress_apply(tokenize_text)\n",
    "profiles['tokens'] = profiles['profile'].progress_apply(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7fb8c8a-3ca1-4e9a-9d3e-6f3deb02b710",
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles = os.path.join('/sise/bshapira-group/lilachzi/models/llm/llama_cpp_python', \n",
    "                        f'ciao_{CATEGORY}', f'{signal}_profiles.csv')\n",
    "profiles = pd.read_csv(profiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69f1cb35-2dc2-44c8-855c-ca89ee569cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8919/8919 [00:05<00:00, 1758.59it/s]\n",
      "100%|██████████| 3805/3805 [00:00<00:00, 4191.29it/s]\n"
     ]
    }
   ],
   "source": [
    "def w2v_vectorize(words):\n",
    "    vectors = []\n",
    "    \n",
    "    for word in words:\n",
    "        if len(word) == 1:\n",
    "            continue\n",
    "        try:    \n",
    "            vectors.append(w2v_model[word])\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return np.average(vectors, axis=0)\n",
    "\n",
    "clean_ciao['embedding'] = clean_ciao['tokens'].progress_apply(w2v_vectorize)\n",
    "profiles['embedding'] = profiles['tokens'].progress_apply(w2v_vectorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "327ea33e-4c50-45d4-a562-30aa783fc18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4269/4269 [00:01<00:00, 3612.03it/s]\n"
     ]
    }
   ],
   "source": [
    "user_features = []\n",
    "for user_idx, user_id in tqdm(users_idx2id.items()):\n",
    "    try:\n",
    "        embeddings = profiles[profiles['user_id'] == user_id]['embedding'].values[0]\n",
    "    except:\n",
    "        embeddings = np.zeros((300, ))\n",
    "    \n",
    "    user_features.append(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e399a27e-68cd-4054-8727-0362fd931f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8919/8919 [00:02<00:00, 3742.75it/s]\n"
     ]
    }
   ],
   "source": [
    "review_features = []\n",
    "for review_idx, review_id in tqdm(reviews_idx2id.items()):\n",
    "    embedding = clean_ciao[clean_ciao['review_id'] == review_id]['embedding'].values[0]\n",
    "    review_features.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1c92b6b-b1ea-4567-a046-51568d15972d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 909/909 [00:00<00:00, 2738.15it/s]\n"
     ]
    }
   ],
   "source": [
    "product_features = []\n",
    "for product_idx, product_id in tqdm(products_idx2id.items()):\n",
    "    embeddings = np.array(clean_ciao[clean_ciao['product_id'] == product_id]['embedding'].tolist())\n",
    "    product_features.append(np.mean(embeddings, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5ce721a3-918d-4226-a151-9e6dbe67d709",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(save_prefix + 'w2v_profiles_implicit_features.npz',\n",
    "         user_features=user_features,\n",
    "         review_features=review_features,\n",
    "         product_features=product_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "18d48409-9d27-467c-b0e7-84eac0173695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/sise/bshapira-group/lilachzi/models/magnn/data/preprocessed/CiaoGames_like_processed/'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859ae535-499b-464f-a071-362d954091cb",
   "metadata": {},
   "source": [
    "# Add info to model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16b02310-5c97-4d1a-bde6-374f786a48a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_model_name(name):\n",
    "    pattern = r'(?P<category>[^_]+)_(?P<sub_feats_type>\\w+_)?(?P<feats_type>\\d+)_(?P<lr>[^_]+)_(?P<weight_decay>[^_]+)_(?P<dropout_rate>[^_]+)_(?P<batch_size>[^_]+)(?P<signal>.+)?'\n",
    "    matches = re.match(pattern, name)\n",
    "\n",
    "    matches = matches.groupdict()\n",
    "    matches['category'] = matches['category'].lstrip('Ciao')\n",
    "    # matches['sub_feats_type'] = matches['sub_feats_type'].rstrip('_') if matches['sub_feats_type'] else matches['sub_feats_type']\n",
    "    matches['signal'] = matches['signal'].lstrip('_') if matches['signal'] else matches['signal']\n",
    "    matches['signal'] = 'like' if not matches['signal'] else matches['signal']\n",
    "\n",
    "    if matches['sub_feats_type'] and any([sig in matches['sub_feats_type'] for sig in ['like', 'write', 'both']]):\n",
    "        signal = matches['sub_feats_type'].split('_')[0]\n",
    "        matches['signal'] = signal\n",
    "        matches['sub_feats_type'] = matches['sub_feats_type'].replace(f'{signal}_', '')\n",
    "\n",
    "    return matches\n",
    "\n",
    "mr = pd.read_csv('/sise/bshapira-group/lilachzi/models/magnn/model_results.csv')\n",
    "mr_info = mr['model'].apply(lambda x: pd.Series(split_model_name(x)))\n",
    "mr = pd.concat([mr_info, mr], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eaee7bd6-30a1-4a65-9788-07061a94bd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr.to_csv('/sise/bshapira-group/lilachzi/models/magnn/model_info_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0eaae1-2ab1-4fc8-9566-eb383382999d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_env",
   "language": "python",
   "name": "llama_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
